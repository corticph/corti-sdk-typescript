/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Corti from "../../../../index.js";

/**
 * @example
 *     {
 *         recordingId: "recordingId",
 *         primaryLanguage: "primaryLanguage",
 *         modelName: "modelName"
 *     }
 */
export interface TranscriptsCreateRequest {
    /** The unique identifier for the recording. */
    recordingId: Corti.Uuid;
    /** The primary spoken language of the recording. Check https://docs.corti.ai/about/languages for more. */
    primaryLanguage: string;
    /** Indicates whether spoken dictation commands should be converted to punctuation (e.g., 'comma' â†’ ','). */
    isDictation?: boolean;
    /** If true, each audio channel is transcribed separately. */
    isMultichannel?: boolean;
    /** If true, separates speakers within an audio channel returning incrementing ids for transcript segments. */
    diarize?: boolean;
    /** An array of participants, each specifying a role and an assigned audio channel in the recording. Leave empty when shouldDiarize: true */
    participants?: Corti.TranscriptsParticipant[];
    /** Can be "base", "enhanced", "premier". By default, only the highest tier is accessible. Check https://docs.corti.ai/about/languages for more. */
    modelName: string;
}
